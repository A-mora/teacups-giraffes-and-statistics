---
title: "Covariance and Correlation"
output:
  bookdown::html_document2:
    includes:
      in_header: covariance_and_correlation_image.html
---

# Gathering data on another variable
Back to the narrative of the teacup giraffes: we now include a 2nd variable
* Is there any relationship to height? Why...
* You wonder how the relationships between 2 variables can be assessed. 

* Say you want to quantify how two variables change together-- one such measure is the **covariance**. 

- The covariance elegantly combines the deviations of observations from two different variables into a single value. 

- We begin with scores representing the distance of each observation from the means of both x and on y. We call these the **deviation** scores on x and on y, respectively. 
  - Deviation scores will take on either a postive or negative sign. 

- Why do we use the deviations? Because we want to answer the question 'do the x values and y values move together with respect their means or not'. 

- We still want to construct shapes with the deviation scores. 

BLIke the variance, the values can fall on either side of the mean... 

- The four things (but more like 3) different things that could happen are:

  - 
  - 
  -
  - quick little thumbnails.

- So we multiply the two deviation scores. This is called the **crossproduct**. 
  + NOTE: We are in the dimension of a squared term. This will become important later. 

<div style="margin-bottom:50px">
</div>
\begin{equation}
 (\#eq:equation1)
 \Large (x_i - \mu_{x})(y_i - \mu_{y})
 \end{equation}
<div style="margin-bottom:50px">
</div>

- SIDE NOTE: 
Let's explore the attributes of the crossproduct. 
  (1) We should note that the signs of the cross product will depend on whether or not the two x- and y- values from the same observation move in the same directions relative to their means. 
  (2)  The magnitude of the cross-product will scale with the absolute value of the deviation scores.
  
- We **sum the crossproducts**: 

<div style="margin-bottom:50px">
</div>
\begin{equation}
 (\#eq:equation2)
 \Large \sum_{i=1}^n (x_i - \mu_x)(y_i - \mu_y)
 \end{equation}
<div style="margin-bottom:50px">
</div>

And then **divide them by N (or N-1 in sample)**  so that we have taken into account how many observations contributed to this quantity. This final value is called the **covariance**. 


<div style="margin-bottom:50px">
</div>
\begin{equation}
 (\#eq:equation3)
 \Large cov = {\frac{1}{N}\sum_{i=1}^n (x_i - \mu_x)(y_i - \mu_y)}
 \end{equation}
<div style="margin-bottom:50px">
</div>


- BUT-- the covariance is not an intuitive value. 
Not only is this a squared term (which makes it difficult to interpret just like the variance was) but it is also is the product of two variables on possibly different scales.

- For example, what would a covariance with units of mm^2* g^2 mean?

- another point to make is that value of the covariance will be vastly different if we had decided to change the units (mm vs cm -- or g vs kg). Quora sentence. 

-As a result, we need to standardize!

# How do we standardize the Covariance?

- We need to rescale the covariance to fall within the boundaries of 0 and maximum spread of the data. 
- The solution is to divide by the product of the standard deviations for each variable (The square root of the variances) -- the resulting value is called the **Pearson correlation coefficient**. The equation is shown below:

<div style="margin-bottom:50px">
</div>
\begin{equation}
 (\#eq:equation1)
 \Large \frac{ \frac{1}{N} \sum_{i=1}^n (x_i - \mu_x)(y_i - \mu_y)}{\sigma_x \sigma_y}
 \end{equation}
<div style="margin-bottom:50px">
</div>

# What the does correlation mean?

# Code it up. 
In the window below, write your own function to compute the sample covariance of two variables, and call it `my_covariance( )`. Then create a second function called `my_correlation( )` in which you will compute the correlation of two variables. You may incorporate your function `my_covariance( )` into this section function to save yourself some steps.

You will need to write both functions so that they will take two parameters. The parameters for `my_covariance( )` have been setup for you.

```{r, include=FALSE}

tutorial::go_interactive(height = 400)
```

```{r ex= "correlation", type="pre-exercise-code"}
set.seed(12)
heights_island1 <- rnorm(50,10,2)
```

```{r ex="correlation", type="sample-code"}

my_covariance <- function(x,y){

  return( )
}

my_correlation <- function(x,y){

 return( )
}

```

```{r ex="correlation", type="solution"}
my_covariance <- function(x,y){
  m_x <- mean(x)
  m_y <- mean(y)

  deviation_x <- x - m_x
  deviation_y <- y - m_y
  
  crossproduct <- deviation_x * deviation_y

  covariance <-  crossproduct / (length(x)-1) # Divides by N-1
  return(covariance)
}

my_correlation <- function(x,y){
 correlation <- my_covariance(x,y) / (sd(x)*sd(y))
 return(correlation)
}

```
```{r ex="variance", type="sct"}

```



# Why divide by $\sigma_x\sigma_y$? 
Well, it's complicated, but it builds on the mathematical principle that the covariance between x and y will never exceed the product of the standard deviations of each variable. This means the maximum  If you don't take our word for it, press play below to see how the relationship between s(x)*s(y) and cov(x,y) looks. 

{INSERT PLOT}

As you look at the plot above, you may have the following questions:
  * Why are there clearly defined boundaries?
    + Because at the edges is wehre the COv is the greatest value it can be-- it is equal to the sxsy there.
    + The slope is 1. 
    
  * Where in the plot/ shape do the strongest correlations end up?
      + On the edges -- when the absolute value of the numerator and denominator of the equation are equal-- the quotient will = 1 (or negative 1, depending on the sign of the covariance in the numerator)
      

# Examples
How we can manipulate different parts of the equation to predictably influence the resulting correlation value. 

Comparing 4 point in perfect correlation, with not perfect

(1) COV DIFF, but STANDARD DEVIATION
(2) COVARIANCE is the SAME/ LARGER DIFF (Desiree's example)
(3) both change (similar to 1 but pushed further...)

# Correlation is not causation
Correlation is not causation. 

# Correlation does not capture relationships that are not linear
If the relationship is not linear, then correlation will not be meaningful.

^^ one of the above can be made into things to think about. 







